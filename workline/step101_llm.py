import pymysql
import re, time, os
import http.client
import json

from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

from typing import Dict, List, Tuple, Optional
from langchain_openai import ChatOpenAI
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from langchain_community.llms import HuggingFacePipeline
import logging

def retry(max_retries=5, delay=12):
    def decorator(func):
        def wrapper(*args, **kwargs):
            attempts = 0
            while attempts < max_retries:
                try:
                    result = func(*args, **kwargs)
                    # Check if the return value contains error information
                    if isinstance(result, dict) and "error" in result:
                        logging.error(f"Error occurred: {result}")
                        # Throw an exception with the error information
                        raise Exception(result["error"]["message"])
                    # If there is no error information, return the normal result
                    return result
                except Exception as e:
                    attempts += 1
                    # Wait for a while and then retry
                    time.sleep(delay)
                    logging.warning(f"Error occurred: {e}. Retrying... (Attempt {attempts}/{max_retries})")
            logging.warning(f"Failed after {max_retries} retries.")
            return None
        return wrapper
    return decorator

def initialize_local_model(model_path: Optional[str] = None, default_model: str = "gpt2") -> HuggingFacePipeline:
    """
    Initialize the local large - language model. If no path is specified, load the default model.

    :param model_path: The folder path of the local model (optional)
    :param default_model: The name of the default model to load (if no path is provided)
    :return: An instance of HuggingFacePipeline
    """
    if model_path:
        # Check if the path exists
        if not os.path.exists(model_path):
            raise ValueError(f"Specified model path does not exist: {model_path}")
        print(f"Loading model from specified path: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path)
    else:
        # Load the default model
        print(f"Model path not provided. Loading default model: {default_model}")
        tokenizer = AutoTokenizer.from_pretrained(default_model)
        model = AutoModelForCausalLM.from_pretrained(default_model)

    # Create a text generation pipeline
    local_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, max_length=512)

    # Wrap it as a LangChain HuggingFacePipeline
    return HuggingFacePipeline(pipeline=local_pipeline)


class ModelExecutor:
    def __init__(self, model_type: str = "local", model_path: Optional[str] = None, api_key: Optional[str] = None,
                 model_name: str = "gpt2"):
        """
        Initialize the model executor, supporting API or offline models.
        :param model_type: The type of the model (e.g., "openai", "local", or "agicto")
        :param model_name: The name of the model (e.g., "gpt - 4", "llama3 - 70b - 8192", or the name of a Hugging Face local model)
        :param api_key: The API key (if using an online model, such as OpenAI or AGICTO)
        """
        self.model_type = model_type
        self.model_name = model_name
        self.api_key = api_key
        self.model_path = model_path

        # Initialize the model
        if self.model_type == "openai":
            if not self.api_key:
                raise ValueError("API key is required for OpenAI models.")
            self.llm = ChatOpenAI(
                base_url='https://api.aigc369.com/v1',
                model_name=self.model_name,
                openai_api_key=self.api_key,
                temperature=0  # Control the stability of generation
            )
        elif self.model_type == "local":
            # Load the local model
            self.llm = initialize_local_model(self.model_path, self.model_name)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

    def execute(self, prompt: str) -> str:
        """
        Execute model inference and return the result.

        :param prompt: The prompt
        :return: The response generated by the model
        """
        if self.model_type == "openai":
            # Call the OpenAI API
            response = self.llm.invoke([HumanMessage(content=prompt)])
            return response.content
        elif self.model_type == "local":
            # Call the local model
            response = self.llm(prompt)
            return response
        else:
            raise ValueError("Invalid model type.")

class ChatModelExecutor:
    def __init__(self, api_url: str, api_key: str, model: str = "gpt - 3.5 - turbo", max_retries=3):
        """
        Initialize the chat model executor.
        :param api_url: The URL address of the API
        :param api_key: The API key
        :param model: The name of the model to use
        :param max_retries: The maximum number of retries
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
        self.max_retries = max_retries  # New: Maximum number of retries

    @retry(max_retries=5, delay=2)
    def send_chat_request(self, messages: List[Dict[str, str]], temperature: float = 0.8, max_tokens: int = 4096) -> str:
        """
        Send a chat request to the specified API.
        :param messages: A list of conversation messages, each message containing a role and content
        :param temperature: Control the randomness of generation
        :param max_tokens: The maximum length of generation
        :return: The response content of the model
        """
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "n": 1,
            "stream": False,
            "top_p": 1,
            "max_tokens": max_tokens,
            "presence_penalty": 1,
            "frequency_penalty": 0
        }

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content - Type': 'application/json'
        }

        conn = http.client.HTTPSConnection(self.api_url)
        conn.request("POST", "/v1/chat/completions", json.dumps(payload), headers)
        res = conn.getresponse()
        data = res.read()
        response = data.decode("utf - 8")
        logging.warning(response)
        try:
            # Parse the response content
            response_data = json.loads(response)
            if "choices" in response_data and len(response_data["choices"]) > 0:
                logging.warning(f"Model Output:\n{response_data['choices'][0]['message']['content']}\n")
                return response_data["choices"][0]["message"]["content"]
            else:
                return "No response from the model."
        except json.JSONDecodeError:
            return f"Error parsing response: {response}"

    def chat(self, user_input: str) -> str:
        """
        Simplify the conversation interface with the model.
        :param user_input: The content input by the user
        :return: The response content of the model
        """
        messages = [
            {"role": "user", "content": user_input}
        ]
        return self.send_chat_request(messages)
        # return self.send_chat_request_with_retries(messages)

def generate_prompt(template: str) -> str:
    """
    Generate a prompt based on the template and parameters.

    :param template: The prompt template
    :param params: A dictionary of parameters used to fill the template
    :return: The generated prompt
    """
    prompt_template = PromptTemplate(template=template)
    return prompt_template.format()


def extract_code(output: str) -> List[str]:
    """
    Extract code snippets from the output of the large model.

    :param output: The output content of the large model
    :return: A list of extracted code snippets
    """
    # Use regular expressions to extract code blocks
    code_blocks = re.findall(r"```(?:python|.*)\n(.*?)```", output, re.DOTALL)
    return [block.strip() for block in code_blocks]


def run_model_task(
        model_executor: ModelExecutor,
        prompt_template: str,
) -> str:
    """
    Run a model task based on the prompt template and parameters, and extract code snippets.

    :param model_executor: An instance of ModelExecutor
    :param prompt_template: The prompt template
    :return: A list of extracted code snippets
    """
    # Execute the model task
    output = model_executor.execute(prompt_template)
    logging.warning(f"Model Output:\n{output}\n")
    # Extract code snippets
    # code_snippets = extract_code(output)
    return output


def read_files_from_directory(directory: str) -> List[Tuple[str, str]]:
    """
    Read all .java and .txt files from the specified directory and match corresponding file pairs.

    :param directory: The folder path
    :return: A list containing (java_file_path, txt_file_path)
    """
    # Get all files in the directory
    files = os.listdir(directory)

    # Filter .java and .txt files
    java_files = {re.match(r"MyJVMTest_(\d+)\.java", f).group(1): os.path.join(directory, f)
                  for f in files if f.endswith(".java") and re.match(r"MyJVMTest_(\d+)\.java", f)}
    txt_files = {re.match(r"MyJVMTest_(\d+)\.txt", f).group(1): os.path.join(directory, f)
                 for f in files if f.endswith(".txt") and re.match(r"MyJVMTest_(\d+)\.txt", f)}

    # Match .java and .txt file pairs
    matched_files = []
    for key in java_files:
        if key in txt_files:
            matched_files.append((java_files[key], txt_files[key]))

    return matched_files


def parse_txt_file(txt_path: str) -> Optional[Tuple[str, int]]:
    """
    Parse data in the format of [library_name, id] from a .txt file.

    :param txt_path: The path of the txt file
    :return: A tuple of (library_name, id)
    """
    with open(txt_path, "r", encoding="utf - 8") as file:
        content = file.read().strip()
        match = re.match(r"\[([\w]+),\s*(\d+)\]", content)
        if match:
            return match.group(1), int(match.group(2))
    return None


def read_java_file(java_path: str) -> str:
    """
    Read the content of a .java file.

    :param java_path: The path of the java file
    :return: The content of the file
    """
    with open(java_path, "r", encoding="utf - 8") as file:
        return file.read().strip()


def construct_sql_query(library_name: str, record_id: int) -> str:
    """
    Construct an SQL query statement.

    :param library_name: The name of the database library
    :param record_id: The ID of the database record
    :return: The constructed SQL query statement
    """
    return f"SELECT * FROM {library_name} WHERE id = {record_id};"


def execute_sql_query(sql_query: str, db_config: Dict[str, str]) -> Optional[List[Dict[str, str]]]:
    """
    Execute an SQL query and return the results.

    :param sql_query: The SQL query statement to execute
    :param db_config: The database configuration, including host, user, password, database, port
    :return: A list of query results, where each result is a dictionary with column names as keys and corresponding values
    """
    try:
        # Connect to the database
        connection = pymysql.connect(
            host=db_config["host"],
            user=db_config["user"],
            password=db_config["password"],
            database=db_config["database"],
            port=db_config.get("port", 3306),  # Default port 3306
            cursorclass=pymysql.cursors.DictCursor  # Return results in dictionary form
        )
        with connection.cursor() as cursor:
            # Execute the query
            cursor.execute(sql_query)
            result = cursor.fetchall()  # Get all query results
        connection.close()
        return result

    except pymysql.MySQLError as e:
        print(f"Error executing SQL query: {e}")
        return None


def generate_prompt_with_query_results(
        java_content: str,
        sql_query: str,
        query_results: Optional[List[Dict[str, str]]]
) -> str:
    """
    Generate a prompt based on the Java file content, SQL query statement, and query results.

    :param java_content: The content of the Java file
    :param sql_query: The SQL query statement
    :param query_results: The query results
    :return: The final generated prompt
    """
    if not query_results:
        results_text = "The query result is empty or the query failed."
    # Format the query results
    else:
        results_text = query_results[0]["content"]

    # Prompt template
    prompt_template = """
    The main task is to generate test cases that can test the JVM, trigger defects in the JVM as much as possible, and cover as many modules as possible.       
    Just output the code. If there is a logical explanation, write it in the Java program as a comment.

    The following is the content of a Java test case file synthesized from a seed and a defect vulnerability code snippet:
    {java_content}

    The above test case was synthesized from the following defect vulnerability code snippet:
    {query_results}

    Please analyze the test case based on the above content and complete the following tasks:
        1. Determine if there is a mismatch between variable types and values, and solve the problem directly.
        2. Determine if there are variable conflicts during the synthesis process, and resolve them properly.
        3. If there are undefined variables or classes, directly supplement the missing context variables to ensure that the test case can be executed directly.
        4. Ensure that the defect trigger code snippet can interact with the seed; otherwise, it's no different from executing two test cases.
        5. Output the organized test case and add a line of comment in the generated Java program to show what you have done.

    """
    return prompt_template.format(
        java_content=java_content,
        query_results=results_text
    )


def process_files_with_query(
        directory: str,
        db_config: Dict[str, str]
) -> List[Dict[str, str]]:
    """
    Process all file pairs in the specified directory and generate prompts containing query results.

    :param directory: The folder path
    :param db_config: The database configuration for executing SQL queries
    :return: A list of dictionaries containing the Java content, SQL query statement, query results, and generated prompts for each file pair
    """
    # Get .java and .txt file pairs
    file_pairs = read_files_from_directory(directory)
    results = []

    for java_path, txt_path in file_pairs:
        # Parse file content
        java_content = read_java_file(java_path)
        txt_data = parse_txt_file(txt_path)

        if txt_data:
            library_name, record_id = txt_data
            sql_query = construct_sql_query(library_name, record_id)

            # Execute the SQL query
            query_results = execute_sql_query(sql_query, db_config)

            # Generate a prompt
            prompt = generate_prompt_with_query_results(java_content, sql_query, query_results)

            # Save the results
            results.append({
                "java_file": java_path,
                "txt_file": txt_path,
                "java_content": java_content,
                "sql_query": sql_query,
                "query_results": query_results,
                "prompt": prompt
            })

    return results


def store_code_results(prompt, code_results, filename='results.txt', delimiter='--- Result ---'):
    # Check if the file exists; if not, create it
    file_exists = os.path.isfile(filename)
    with open(filename, 'a', encoding='utf - 8', errors='replace') as file:
        if not file_exists:
            # If the file does not exist, write the results directly
            file.write(code_results + '\n')
        else:
            # If the file exists, add a delimiter and the results
            # Calculate the number of results in the current file
            with open(filename, 'r', encoding='utf - 8', errors='replace') as read_file:
                existing_content = read_file.read()
                result_count = existing_content.count(delimiter)  # Calculate the number of results based on the delimiter
            # Write the delimiter and the results
            file.write(delimiter)
            file.write("\n--- {} ---\n".format(result_count))
            file.write(code_results + '\n')

# Build a list of target paths
def build_target_paths(root_dir):
    target_paths = []
    # Traverse all subdirectories under
    for sub_dir in os.listdir(root_dir):

        if os.path.isdir(os.path.join(root_dir, sub_dir)):

            full_path = os.path.join(root_dir, sub_dir, "iteration_1", "target_testsuits", "1")

            if os.path.exists(full_path):
                target_paths.append(full_path)
    return target_paths


if __name__ == "__main__":
    count = 0

    start_time = time.time()
    # mode_name = "llama-3.1-405b-instruct"
    # mode_name = "doubao-lite-128k"
    # mode_name = "moonshot-v1-8k"
    # mode_name = "deepseek-v3"
    mode_name = "claude-3-5-sonnet-20240620"


    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler("{}.log".format(mode_name.replace("/","-")), mode="a", encoding="utf-8"),
            logging.StreamHandler() 
        ]
    )
    try:
        chat_executor = ChatModelExecutor(
            # api_url="api.huiyan-ai.cn",
            api_url="xgapi.ihy.ai",
            api_key="sk-562b8FktEaoSODPZ5k3clTFucZ0k8H2gKJHyPiAqY8pyoD3R",
            model=mode_name,
            max_retries=5
        )
        executor = ModelExecutor(
            model_type="openai",
            model_name=mode_name,  # gpt-4o gpt-3.5-turbo
            api_key="sk-U8SRnh5HsHLaf9NdVMH8NUJHSJVdac9EWQgO4ih4O28Jkmhh"
        )

        db_config = {
            "host": "10.15.0.38",
            "user": "root",
            "password": "root",
            "database": "ssfuzzTest",
            "port": 10348
        }

        root_directory = r"H:\Projects\pythonProject\ssfuzz\parser\feature\src\test\output\task-2025-01-21_12-14-25"

        target_paths = build_target_paths(root_directory)
        for directory_path in target_paths:

            processed_results = process_files_with_query(directory_path, db_config)

            for result in processed_results:
                count += 1
                logging.info("-" * 80 + str(count) + "-" * 80)
                template = result['prompt']

                logging.info(f"prompt：{template}")

                code_results = chat_executor.chat(template)

                filename = "llm/log/{}-result.txt".format(mode_name.replace("/","-"))
                store_code_results(result['prompt'], code_results, filename=filename)

    except Exception as e:
        logging.error(f"error：{e}")
    finally:
        pass
